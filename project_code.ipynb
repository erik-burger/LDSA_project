{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to spark cluster\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.118:7077\") \\\n",
    "        .appName(\"hadoop_example\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from the HDFS cluster.\n",
    "rdd = spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.118:9000/unzip_input/RC_2011-09',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text'\n",
    ")\n",
    "# for weak scalcability test increase no_duplicates otherwise keep no_duplicates = 0\n",
    "# no_duplicates = 0 for one node \n",
    "# no_duplicates = 1 for two nodes\n",
    "# no_duplicates = 2 for three nodes\n",
    "# no_duplicates = 3 for four nodes\n",
    "no_duplicates=0\n",
    "for i in range(no_duplicates):\n",
    "    rdd += spark_context.newAPIHadoopFile(\n",
    "    'hdfs://192.168.2.118:9000/unzip_input/RC_2011-09',\n",
    "    'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import re\n",
    "\n",
    "# Convert each line of the file into json format and then filter out lines only from the subbredit AskReddit\n",
    "import json\n",
    "rdd_askReddit = rdd\\\n",
    "    .map(lambda line: json.loads(line[1]))\\\n",
    "    .filter(lambda line: line[\"subreddit\"] == \"AskReddit\")\n",
    "\n",
    "# Slit the comments on cahracters that aren't letters, numbers or '. \n",
    "# Then maps all words into one list insted of one for every comment.\n",
    "# Removes words that are empty (\"\").\n",
    "# Then does a word count using map and reduce.\n",
    "r = re.compile(r\"[^a-zA-Z0-9']\")\n",
    "rdd_word_count = rdd_askReddit\\\n",
    "    .map(lambda line: r.split(line[\"body\"]))\\\n",
    "    .flatMap(lambda word_list: list(word_list))\\\n",
    "    .filter(lambda word: word != '')\\\n",
    "    .map(lambda word: (word.lower(), 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1845776), ('i', 1449640), ('to', 1394168), ('a', 1368250), ('and', 1191115), ('you', 882045), ('of', 881621), ('it', 763788), ('that', 720639), ('in', 686286), ('is', 599635), ('for', 458880), ('my', 450864), ('was', 398039), ('with', 355510), ('but', 354209), ('on', 353653), ('have', 350931), ('not', 333752), ('this', 325213), ('be', 324019), ('if', 301716), ('are', 295334), ('me', 272701), ('your', 270539), ('just', 260878), ('as', 253776), ('they', 251298), ('or', 242290), ('like', 238776), ('so', 228956), ('at', 227886), ('do', 206153), ('about', 200973), ('out', 200529), (\"don't\", 199893), ('he', 198363), (\"it's\", 196166), ('what', 193391), ('all', 190255), ('up', 186517), ('people', 182483), ('get', 180860), ('would', 180676), ('when', 178846), (\"i'm\", 178807), ('one', 177993), ('can', 168212), ('an', 156360), ('from', 154484), ('we', 151916), ('her', 151517), ('no', 151152), ('there', 147261), ('deleted', 144910), ('them', 142506), ('she', 141771), ('had', 134432), ('think', 130805), ('time', 129852), ('because', 129620), ('some', 127957), ('know', 126138), ('more', 124811), ('will', 120177), ('then', 118937), ('how', 118884), ('who', 116250), ('really', 112408), ('by', 108865), ('http', 105458), ('his', 98647), ('go', 98526), ('good', 97923), ('their', 97355), ('him', 96801), ('were', 90765), ('make', 89066), ('com', 87981), (\"you're\", 87707), ('only', 86471), ('than', 86118), ('other', 85760), ('has', 84773), ('want', 82283), ('even', 81663), ('much', 81498), ('also', 81417), ('way', 80312), ('something', 78776), ('being', 78308), ('now', 77548), ('could', 77132), ('say', 76601), ('never', 76357), ('well', 74518), ('into', 73946), ('going', 73758), ('too', 71895), ('any', 71568)]\n"
     ]
    }
   ],
   "source": [
    "# Measure te runtime of the application usin time and also outputs the top 100 most common word in the Subreddit Askreddit\n",
    "import time\n",
    "start = time.time()\n",
    "print(rdd_word_count.takeOrdered(100, key=lambda x: -x[1]))\n",
    "end = time.time()\n",
    "runtime = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104.96110558509827\n"
     ]
    }
   ],
   "source": [
    "# Print out the runtime to be able to perfor scalability tests\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
